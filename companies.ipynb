{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df6d39d7-63a9-4969-9a80-06c3408bbf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyalex import (\n",
    "    Works, Authors, Sources,\n",
    "    Institutions, Concepts, Publishers, Funders\n",
    ")\n",
    "import pyalex\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pyalex.config.email = \"david@rs21.io\"\n",
    "\n",
    "### ssl problems\n",
    "# https://support.chainstack.com/hc/en-us/articles/9117198436249-Common-SSL-Issues-on-Python-and-How-to-Fix-it\n",
    "\n",
    "from flair.embeddings import DocumentPoolEmbeddings\n",
    "from flair.data import Sentence\n",
    "from flair.embeddings import SentenceTransformerDocumentEmbeddings\n",
    "\n",
    "EMBEDDING_MODEL_1 = \"all-mpnet-base-v2\" \n",
    "\n",
    "# this one is also good: all-MiniLM-L6-v2\n",
    "EMBEDDING_MODEL_2 = \"all-MiniLM-L6-v2\"\n",
    "SENT_EMBEDDINGS_1 = SentenceTransformerDocumentEmbeddings(EMBEDDING_MODEL_1)\n",
    "SENT_EMBEDDINGS_2 = SentenceTransformerDocumentEmbeddings(EMBEDDING_MODEL_2)\n",
    "DOC_EMBEDDINGS= DocumentPoolEmbeddings([SENT_EMBEDDINGS_2])\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import yake\n",
    "import umap.umap_ as umap\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.mixture import GaussianMixture as GMM\n",
    "import altair as alt\n",
    "import math\n",
    "import plotly.express as px\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf103272-770e-482f-b915-ec39429916c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_works_list(worklist:list):\n",
    "    \"\"\"\n",
    "    transforms the \n",
    "    works list into a dataframe.\n",
    "    \"\"\"\n",
    "    abstracts_dict = {h[\"id\"]:h[\"abstract\"] for h in worklist}\n",
    "    df = pd.DataFrame.from_records(worklist)\n",
    "    try: \n",
    "        del df['abstract_inverted_index'] # though don't all have abstracts is the problem\n",
    "        df['abstract'] = df['id'].map(abstracts_dict)\n",
    "    except:\n",
    "        pass\n",
    "   # df['author_affils'] = df['authorships'].apply(get_authors_and_affils)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ca597d3-d50b-4334-8030-39e67621c553",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyalex import config\n",
    "\n",
    "config.max_retries = 0\n",
    "config.retry_backoff_factor = 0.1\n",
    "config.retry_http_codes = [429, 500, 503]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e1ad01-6f9b-4b26-89f3-d52a33f5940e",
   "metadata": {},
   "source": [
    "https://github.com/J535D165/pyalex\n",
    "\n",
    "How to get all the works from US companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27760e5e-cdc4-4296-a00d-c529f2f48f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Works().random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62931672-434b-4982-a584-c26e5f321adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Works().filter(authorships={\"institutions\": {\"country_code\": \"US\",\n",
    "#                                             \"type\": \"company\"}}).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28d2081a-c21f-4449-9054-8abd06fc91e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_country_type_frame(country_code:str, affil_type:str):\n",
    "    \"\"\"\n",
    "    takes a country_code and affil_type\n",
    "    and forms the pagination object to retrive the \n",
    "    records\n",
    "    \"\"\"\n",
    "    pager = Works().filter(publication_year = '>2021',\n",
    "        authorships={\"institutions\": {\"country_code\": country_code,\n",
    "                        \"type\": affil_type}}).paginate(\n",
    "        per_page=200, n_max=100_000)\n",
    "   # pager = Works().filter(publication_year='>2018',\n",
    "   # authorships={\"institutions\": {\"ror\": ror}}).paginate(\n",
    "   #     per_page=200, n_max=None)\n",
    "    #pager = Works().filter(publication_year='>2016',\n",
    "    #concepts={\"id\":f\"{concepts_list[i]['id']}\"}).filter(authorships={\"institutions\":{\"country_code\":\"CN\"}}).\\\n",
    "    #paginate(per_page=200,n_max=None)\n",
    "    #concepts={\"id\":f\"{concepts_list[i]['id']}\"}).\\\n",
    "    #paginate(per_page=200,n_max=None)\n",
    "    df = pd.DataFrame()\n",
    "    for page in tqdm(pager):\n",
    "        dfpage = process_works_list(page)\n",
    "        df = pd.concat([df, dfpage], ignore_index=True)\n",
    "        df.drop_duplicates(subset='id', keep='first',inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "939237b6-d354-4317-8a78-9e9309a242ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500it [23:05,  2.77s/it]\n"
     ]
    }
   ],
   "source": [
    "df = get_country_type_frame(country_code=\"US\", affil_type=\"company\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e2c857c0-8545-4f22-a667-ce8e709ee069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 49)\n"
     ]
    }
   ],
   "source": [
    "#dftop = pd.concat(df,\n",
    "#                  ignore_index=True)\n",
    "dftop = df\n",
    "dftop.drop_duplicates(subset='id', keep='first', \n",
    "                      inplace=True)\n",
    "\n",
    "dftop.set_index('id', inplace=True, drop=False)\n",
    "\n",
    "dfall = dftop\n",
    "print(dfall.shape)\n",
    "\n",
    "dfall['content'] = dfall['title'] + \". \" + dfall['abstract']\n",
    "\n",
    "dfrecords = dfall[~dfall['content'].isna()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c6db159d-25c3-4d4f-a3ae-78e7e8f3c41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords(text:str, top:int=7, stopwords=None):\n",
    "    \"\"\"\n",
    "    takes a blob of text and \n",
    "    returns the top **top** \n",
    "    keywords as a list\n",
    "    \"\"\"\n",
    "    kw_extractor = yake.KeywordExtractor(top=top, stopwords=stopwords)\n",
    "    keywords = kw_extractor.extract_keywords(text)\n",
    "    return [p[0] for p in keywords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b3503762-b81a-4b57-a082-a48487834732",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_concepts(concept_list:list,score:float=.6):\n",
    "    \"\"\"\n",
    "    takes a list of concept dictionaries \n",
    "    returns the top **top** display_names;\n",
    "    concepts whose score is >= score\n",
    "    \"\"\"\n",
    "    return [c['display_name'] for c in concept_list if c['score'] >= score]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5ac4bb92-7aa6-406b-bc0f-24fd67c05de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfrecords['keywords'] = dfrecords['content'].apply(get_keywords)\n",
    "dfrecords['top_concepts'] = dfrecords['concepts'].apply(get_top_concepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4d84a95f-443d-48af-b96d-44015899494c",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = dfrecords['content'].str.lower().values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3f5950f8-6498-4c65-ab44-1afa15b57d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content_embeddings(dfrecords:pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    passes the preprocessed mitigation strings\n",
    "    data through the embedding model to produce the vector\n",
    "    space representation of each pet mitigation.\n",
    "    \"\"\"\n",
    "    sent = Sentence(\"The grass is green.\")\n",
    "    DOC_EMBEDDINGS.embed(sent)\n",
    "    texts = dfrecords[\"content\"].str.lower().values.tolist()\n",
    "    all_descriptions = np.empty((len(texts), len(sent.embedding)))\n",
    "    for i in tqdm(range(len(texts))):\n",
    "        sent = Sentence(texts[i])\n",
    "        DOC_EMBEDDINGS.embed(sent)\n",
    "        all_descriptions[i, :] = sent.embedding.cpu().numpy()\n",
    "        # gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    dfcontentvectors = pd.DataFrame.from_records(all_descriptions, index=dfrecords.index)\n",
    "    return dfcontentvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6ebef5c1-0870-4495-ad97-7f758e8e1c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 92733/92733 [57:26<00:00, 26.90it/s]\n"
     ]
    }
   ],
   "source": [
    "dfcontentvectors = get_content_embeddings(dfrecords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b9e45e20-dc47-4e0b-bc97-cef0bce16eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/davidd/.local/lib/python3.10/site-packages/umap/umap_.py:1943: UserWarning: n_jobs value -1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(f\"n_jobs value {self.n_jobs} overridden to 1 by setting random_state. Use no seed for parallelism.\")\n"
     ]
    }
   ],
   "source": [
    "#umap.UMAP?\n",
    "N_COMPONENTS = 2 # can visualize this way\n",
    "umap_reducer = umap.UMAP(n_components=N_COMPONENTS,\n",
    "                       #  metric='euclidean')\n",
    "                         random_state=1234,\n",
    "                         metric='cosine')  # can experiment with this metric as well as the other \n",
    "# parameters\n",
    "# to see what other literature is in the same information space, we need to keep this umap_reducer \n",
    "# object as well as the gmm model below.\n",
    "\n",
    "# Apply UMAP to the vectorized strings\n",
    "reduced_vectors = umap_reducer.fit_transform(dfcontentvectors.to_numpy())\n",
    "dfreduced = pd.DataFrame.from_records(reduced_vectors, \n",
    "                index=dfcontentvectors.index)\n",
    "dfreduced.columns = ['x','y']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39da748-8b13-45bf-a4cf-3115bfe5820d",
   "metadata": {},
   "source": [
    "# use hdbscan to cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "18ae14b2-66f1-4a7d-a7ed-80b99c89e4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "import hdbscan\n",
    "\n",
    "hdbscan_args = {'min_cluster_size': 15,\n",
    "                            'metric': 'euclidean',\n",
    "                            'cluster_selection_method': 'eom',\n",
    "                            'cluster_selection_epsilon': 0.1\n",
    "               }\n",
    "\n",
    "cluster = hdbscan.HDBSCAN(**hdbscan_args).fit(dfreduced[['x','y']].to_numpy())\n",
    "\n",
    "dfreduced['cluster'] = cluster.labels_\n",
    "dfreduced['probability'] = cluster.probabilities_\n",
    "\n",
    "dfpapers = dfrecords.merge(dfreduced, left_index=True,\n",
    "                           right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5b45715c-5924-4a5a-b00a-1021017b22da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>doi</th>\n",
       "      <th>title</th>\n",
       "      <th>display_name</th>\n",
       "      <th>publication_year</th>\n",
       "      <th>publication_date</th>\n",
       "      <th>ids</th>\n",
       "      <th>language</th>\n",
       "      <th>primary_location</th>\n",
       "      <th>type</th>\n",
       "      <th>...</th>\n",
       "      <th>created_date</th>\n",
       "      <th>fulltext_origin</th>\n",
       "      <th>is_authors_truncated</th>\n",
       "      <th>abstract</th>\n",
       "      <th>content</th>\n",
       "      <th>top_concepts</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>cluster</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://openalex.org/W3201073812</td>\n",
       "      <td>https://doi.org/10.1016/j.cpc.2021.108171</td>\n",
       "      <td>LAMMPS - a flexible simulation tool for partic...</td>\n",
       "      <td>LAMMPS - a flexible simulation tool for partic...</td>\n",
       "      <td>2022</td>\n",
       "      <td>2022-02-01</td>\n",
       "      <td>{'openalex': 'https://openalex.org/W3201073812...</td>\n",
       "      <td>en</td>\n",
       "      <td>{'is_oa': True, 'landing_page_url': 'https://d...</td>\n",
       "      <td>article</td>\n",
       "      <td>...</td>\n",
       "      <td>2021-09-27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Since the classical molecular dynamics simulat...</td>\n",
       "      <td>LAMMPS - a flexible simulation tool for partic...</td>\n",
       "      <td>[Python (programming language), Computer science]</td>\n",
       "      <td>-2.751676</td>\n",
       "      <td>6.058049</td>\n",
       "      <td>76</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://openalex.org/W4312443924</td>\n",
       "      <td>https://doi.org/10.1109/cvpr52688.2022.01167</td>\n",
       "      <td>A ConvNet for the 2020s</td>\n",
       "      <td>A ConvNet for the 2020s</td>\n",
       "      <td>2022</td>\n",
       "      <td>2022-06-01</td>\n",
       "      <td>{'openalex': 'https://openalex.org/W4312443924...</td>\n",
       "      <td>en</td>\n",
       "      <td>{'is_oa': False, 'landing_page_url': 'https://...</td>\n",
       "      <td>article</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-01-04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The “Roaring 20s” of visual recognition began ...</td>\n",
       "      <td>A ConvNet for the 2020s. The “Roaring 20s” of ...</td>\n",
       "      <td>[Transformer, Computer science, Artificial int...</td>\n",
       "      <td>3.997951</td>\n",
       "      <td>7.043410</td>\n",
       "      <td>76</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://openalex.org/W4226236384</td>\n",
       "      <td>https://doi.org/10.1056/nejmoa2118542</td>\n",
       "      <td>Oral Nirmatrelvir for High-Risk, Nonhospitaliz...</td>\n",
       "      <td>Oral Nirmatrelvir for High-Risk, Nonhospitaliz...</td>\n",
       "      <td>2022</td>\n",
       "      <td>2022-04-14</td>\n",
       "      <td>{'openalex': 'https://openalex.org/W4226236384...</td>\n",
       "      <td>en</td>\n",
       "      <td>{'is_oa': True, 'landing_page_url': 'https://d...</td>\n",
       "      <td>article</td>\n",
       "      <td>...</td>\n",
       "      <td>2022-05-05</td>\n",
       "      <td>pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nirmatrelvir is an orally administered severe ...</td>\n",
       "      <td>Oral Nirmatrelvir for High-Risk, Nonhospitaliz...</td>\n",
       "      <td>[Medicine, Coronavirus disease 2019 (COVID-19)...</td>\n",
       "      <td>3.190367</td>\n",
       "      <td>-1.239761</td>\n",
       "      <td>44</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://openalex.org/W4283271244</td>\n",
       "      <td>https://doi.org/10.1038/s41375-022-01613-1</td>\n",
       "      <td>The 5th edition of the World Health Organizati...</td>\n",
       "      <td>The 5th edition of the World Health Organizati...</td>\n",
       "      <td>2022</td>\n",
       "      <td>2022-06-22</td>\n",
       "      <td>{'openalex': 'https://openalex.org/W4283271244...</td>\n",
       "      <td>en</td>\n",
       "      <td>{'is_oa': True, 'landing_page_url': 'https://d...</td>\n",
       "      <td>article</td>\n",
       "      <td>...</td>\n",
       "      <td>2022-06-23</td>\n",
       "      <td>pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The upcoming 5th edition of the World Health O...</td>\n",
       "      <td>The 5th edition of the World Health Organizati...</td>\n",
       "      <td>[Histiocyte, Myeloid]</td>\n",
       "      <td>1.420206</td>\n",
       "      <td>-4.142851</td>\n",
       "      <td>76</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://openalex.org/W4310461604</td>\n",
       "      <td>https://doi.org/10.1056/nejmoa2212948</td>\n",
       "      <td>Lecanemab in Early Alzheimer’s Disease</td>\n",
       "      <td>Lecanemab in Early Alzheimer’s Disease</td>\n",
       "      <td>2023</td>\n",
       "      <td>2023-01-05</td>\n",
       "      <td>{'openalex': 'https://openalex.org/W4310461604...</td>\n",
       "      <td>en</td>\n",
       "      <td>{'is_oa': True, 'landing_page_url': 'https://d...</td>\n",
       "      <td>article</td>\n",
       "      <td>...</td>\n",
       "      <td>2022-12-10</td>\n",
       "      <td>pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The accumulation of soluble and insoluble aggr...</td>\n",
       "      <td>Lecanemab in Early Alzheimer’s Disease. The ac...</td>\n",
       "      <td>[Medicine, Disease, Alzheimer's disease]</td>\n",
       "      <td>-1.830902</td>\n",
       "      <td>0.093264</td>\n",
       "      <td>76</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id  \\\n",
       "0  https://openalex.org/W3201073812   \n",
       "1  https://openalex.org/W4312443924   \n",
       "2  https://openalex.org/W4226236384   \n",
       "3  https://openalex.org/W4283271244   \n",
       "4  https://openalex.org/W4310461604   \n",
       "\n",
       "                                            doi  \\\n",
       "0     https://doi.org/10.1016/j.cpc.2021.108171   \n",
       "1  https://doi.org/10.1109/cvpr52688.2022.01167   \n",
       "2         https://doi.org/10.1056/nejmoa2118542   \n",
       "3    https://doi.org/10.1038/s41375-022-01613-1   \n",
       "4         https://doi.org/10.1056/nejmoa2212948   \n",
       "\n",
       "                                               title  \\\n",
       "0  LAMMPS - a flexible simulation tool for partic...   \n",
       "1                            A ConvNet for the 2020s   \n",
       "2  Oral Nirmatrelvir for High-Risk, Nonhospitaliz...   \n",
       "3  The 5th edition of the World Health Organizati...   \n",
       "4             Lecanemab in Early Alzheimer’s Disease   \n",
       "\n",
       "                                        display_name  publication_year  \\\n",
       "0  LAMMPS - a flexible simulation tool for partic...              2022   \n",
       "1                            A ConvNet for the 2020s              2022   \n",
       "2  Oral Nirmatrelvir for High-Risk, Nonhospitaliz...              2022   \n",
       "3  The 5th edition of the World Health Organizati...              2022   \n",
       "4             Lecanemab in Early Alzheimer’s Disease              2023   \n",
       "\n",
       "  publication_date                                                ids  \\\n",
       "0       2022-02-01  {'openalex': 'https://openalex.org/W3201073812...   \n",
       "1       2022-06-01  {'openalex': 'https://openalex.org/W4312443924...   \n",
       "2       2022-04-14  {'openalex': 'https://openalex.org/W4226236384...   \n",
       "3       2022-06-22  {'openalex': 'https://openalex.org/W4283271244...   \n",
       "4       2023-01-05  {'openalex': 'https://openalex.org/W4310461604...   \n",
       "\n",
       "  language                                   primary_location     type  ...  \\\n",
       "0       en  {'is_oa': True, 'landing_page_url': 'https://d...  article  ...   \n",
       "1       en  {'is_oa': False, 'landing_page_url': 'https://...  article  ...   \n",
       "2       en  {'is_oa': True, 'landing_page_url': 'https://d...  article  ...   \n",
       "3       en  {'is_oa': True, 'landing_page_url': 'https://d...  article  ...   \n",
       "4       en  {'is_oa': True, 'landing_page_url': 'https://d...  article  ...   \n",
       "\n",
       "  created_date fulltext_origin is_authors_truncated  \\\n",
       "0   2021-09-27             NaN                  NaN   \n",
       "1   2023-01-04             NaN                  NaN   \n",
       "2   2022-05-05             pdf                  NaN   \n",
       "3   2022-06-23             pdf                  NaN   \n",
       "4   2022-12-10             pdf                  NaN   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  Since the classical molecular dynamics simulat...   \n",
       "1  The “Roaring 20s” of visual recognition began ...   \n",
       "2  Nirmatrelvir is an orally administered severe ...   \n",
       "3  The upcoming 5th edition of the World Health O...   \n",
       "4  The accumulation of soluble and insoluble aggr...   \n",
       "\n",
       "                                             content  \\\n",
       "0  LAMMPS - a flexible simulation tool for partic...   \n",
       "1  A ConvNet for the 2020s. The “Roaring 20s” of ...   \n",
       "2  Oral Nirmatrelvir for High-Risk, Nonhospitaliz...   \n",
       "3  The 5th edition of the World Health Organizati...   \n",
       "4  Lecanemab in Early Alzheimer’s Disease. The ac...   \n",
       "\n",
       "                                        top_concepts         x         y  \\\n",
       "0  [Python (programming language), Computer science] -2.751676  6.058049   \n",
       "1  [Transformer, Computer science, Artificial int...  3.997951  7.043410   \n",
       "2  [Medicine, Coronavirus disease 2019 (COVID-19)...  3.190367 -1.239761   \n",
       "3                              [Histiocyte, Myeloid]  1.420206 -4.142851   \n",
       "4           [Medicine, Disease, Alzheimer's disease] -1.830902  0.093264   \n",
       "\n",
       "  cluster probability  \n",
       "0      76         1.0  \n",
       "1      76         1.0  \n",
       "2      44         1.0  \n",
       "3      76         1.0  \n",
       "4      76         1.0  \n",
       "\n",
       "[5 rows x 55 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#help(dfpapers.explode)\n",
    "del dfpapers['id']\n",
    "dfstart = dfpapers.reset_index()\n",
    "dfstart.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e8481204-3f02-4adc-a9ea-c2cd54456794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2022    33223\n",
       "2024    32322\n",
       "2023    27184\n",
       "2025        4\n",
       "Name: publication_year, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfstart['publication_year'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "67411287-65b8-4a00-8729-e2f2df41b069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(92733, 55)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfstart.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "52a65041-d7a2-45f8-a3a7-01aac2f9edc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1028780, 55), (92733, 55))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfbig = dfstart.explode(column='authorships')\n",
    "dfbig.shape, dfstart.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "841de122-a615-4ebb-931b-07642b2ea598",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_extra_to_authorships(row: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    row[authorships] is a dictionary;\n",
    "    add in the id key to that dictionary\n",
    "    whose value is row[id]\n",
    "    \"\"\"\n",
    "    complete_dict = row[\"authorships\"]\n",
    "   # assert type(complete_dict) == dict\n",
    "    #print(type(complete_dict))\n",
    "    if type(complete_dict) == dict:\n",
    "        complete_dict[\"id\"] = row[\"id\"]\n",
    "        complete_dict[\"x\"] = row[\"x\"]\n",
    "        complete_dict[\"y\"] = row[\"y\"]\n",
    "        complete_dict[\"cluster\"] = row[\"cluster\"]\n",
    "        complete_dict[\"cluster_score\"] = row[\"probability\"]\n",
    "        complete_dict[\"title\"] = row[\"title\"]\n",
    "        complete_dict[\"abstract\"] = row[\"abstract\"]\n",
    "        complete_dict[\"doi\"] = row[\"doi\"]\n",
    "        complete_dict[\"publication_date\"] = row[\"publication_date\"]\n",
    "        complete_dict[\"publication_year\"] = row[\"publication_year\"]\n",
    "        complete_dict[\"grants\"] = row[\"grants\"]\n",
    "        complete_dict[\"locations\"] = row[\"locations\"]\n",
    "        return complete_dict\n",
    "    else:\n",
    "        return row[\"authorships\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e45bacad-060d-4ed6-b43d-78f1c2c32533",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfbig['big_authorships'] = dfbig.apply(add_extra_to_authorships, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7c6c30fb-4bda-44ed-ab4e-5e516e3b6639",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfbig['authorships'].tolist()\n",
    "bigvals = dfbig['authorships'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "39ecd0ef-98b1-4bb9-aa99-18465476de2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictvals = [c for c in bigvals if type(c) != float]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f2de63bb-a821-4a28-8209-ab0451e57f26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['id', 'display_name', 'orcid'])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictvals[0]['author'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3ab679ca-cfaa-494f-a7fd-1e3a61622e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "dftriple = pd.json_normalize(dictvals,\n",
    "                  record_path=['institutions'],\n",
    "                  meta=['id','raw_affiliation_strings','author_position', 'doi',\n",
    "                        'title','abstract','publication_date', 'publication_year',\n",
    "                        'grants','locations',\n",
    "                        'is_corrresponding','x','y','cluster','cluster_score',\n",
    "                       ['author','id'], ['author', 'display_name'],\n",
    "                       ['author','orcid']],\n",
    "                  errors='ignore',\n",
    "                  sep='_',\n",
    "                  meta_prefix='paper_',\n",
    "                #  record_prefix='author_'\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "70450dfe-eb59-47a9-a01b-548ea3d80c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dftopics = dfcontentvectors.copy()\n",
    "dftopics['cluster'] = dfpapers['cluster']\n",
    "dfmeantopics = dftopics.groupby('cluster').mean().copy()\n",
    "reduced_topics = umap_reducer.transform(dfmeantopics.to_numpy())\n",
    "df_reduced_topics = pd.DataFrame.from_records(reduced_topics, \n",
    "                index=dfmeantopics.index)\n",
    "df_reduced_topics.columns = ['x','y']\n",
    "df_reduced_topics['topic'] = df_reduced_topics.index\n",
    "df_reduced_topics.head()\n",
    "\n",
    "def get_cluster_concepts(topic_num:int, n:int=20):\n",
    "    \"\"\"\n",
    "    takes an integer topic_num corresponding to a \n",
    "    given topic number and\n",
    "    returns the list of top n occuring concepts\n",
    "    from the top_concept field\n",
    "    \"\"\"\n",
    "    top_concepts = dfpapers[dfpapers['cluster'] == topic_num]['top_concepts'].tolist()\n",
    "    flat_concepts = [item for sublist in top_concepts for item in sublist]\n",
    "    concepts_dict = {c:flat_concepts.count(c) for c in flat_concepts}\n",
    "    sorted_concepts = sorted(concepts_dict.items(), key=lambda x:x[1], reverse=True)\n",
    "    return [c[0] for c in sorted_concepts][:n]\n",
    "\n",
    "def get_yake_cluster_phrases(topic_num:int, n:int=20):\n",
    "    \"\"\"\n",
    "    takes in an integer n corresponding\n",
    "    to a given topic number and\n",
    "    returns the list of keyphrases (TopicRank method)\n",
    "    \"\"\"\n",
    "    documents = dfpapers[dfpapers['cluster'] == topic_num]['content'].tolist()\n",
    "    topic_input = \". \".join(documents)\n",
    "    #extractor = pke.unsupervised.TextRank()\n",
    "    kw_extractor = yake.KeywordExtractor(top=n, stopwords=None)\n",
    "    keywords = kw_extractor.extract_keywords(topic_input)\n",
    "    #extractor.load_document(input=topic_input,\n",
    "    #                    language='en',\n",
    "    #                    normalization=None)\n",
    "\n",
    "    #extractor.candidate_selection()\n",
    "\n",
    "    #window = 2\n",
    "    #use_stems = False\n",
    "    #extractor.candidate_weighting(window=window,\n",
    "    #                          use_stems=use_stems)\n",
    "    #extractor.candidate_weighting()\n",
    "    #threshold = 0.8\n",
    "   # keyphrases = extractor.get_n_best(n=20, threshold=threshold)\n",
    "    #keyphrases = extractor.get_n_best(n=n)\n",
    "    return [p[0] for p in keywords]\n",
    "\n",
    "wikiconcepts = df_reduced_topics['topic'].apply(get_cluster_concepts)\n",
    "\n",
    "wikikeywords = df_reduced_topics['topic'].apply(get_yake_cluster_phrases)\n",
    "\n",
    "dfpapers['id'] = dfpapers.index\n",
    "dfinfo = dfpapers[['x','y','id','title','doi','cluster','grants',\n",
    "                   'locations',\n",
    "                 'publication_date','keywords','top_concepts']].copy()\n",
    "\n",
    "centroids = dfinfo.groupby('cluster')[['x','y']].mean().copy()\n",
    "centroids['concepts'] = wikiconcepts\n",
    "centroids['cluster'] = centroids.index\n",
    "centroids['keywords'] = wikikeywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728621e9-3e17-4f4e-aa76-8694e322e6b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
